





import os
import sys
#-------------------------------------------------------
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
import jax.numpy as jnp
import jax
#-------------------------------------------------------
import sklearn.manifold
import sklearn.cluster
import rdkit
import rdkit.Chem
import rdkit.Chem.Draw
#-------------------------------------------------------
from matplotlib.axes import Axes
from rdkit.Chem import Mol
from pandas import DataFrame, Index
from typing import List, Callable, Union, Tuple
from jaxlib.xla_extension import ArrayImpl
#-------------------------------------------------------
os.environ['SUP_LEARN_PYTHON_DIR_PATH'] =  os.path.join(os.getcwd(), '../src')
os.environ['SUP_LEARN_DATA_DIR_PATH'] =  os.path.join(os.getcwd(), '../data')
sys.path.append(os.getenv('SUP_LEARN_PYTHON_DIR_PATH'))
sys.path.append(os.getenv('SUP_LEARN_DATA_DIR_PATH'))

from utils import linear_regression_utils





aq_sol_dataset: DataFrame = pd.read_csv(
    os.path.join(
        os.getenv('SUP_LEARN_DATA_DIR_PATH'), 
        'raw/aqsoldb-solubility-dataset.csv'
    )
)
aq_sol_dataset.head()








# Plot histogram with density=True to get the probability density
plt.hist(aq_sol_dataset.Solubility, bins=30, density=True, alpha=0.6, color='g', edgecolor='black')

# Overlay a KDE line
sns.kdeplot(aq_sol_dataset.Solubility, color='blue')

# Add labels and title
plt.xlabel('Solubility/LogS')
plt.ylabel('Probability Density')
plt.title('AqSolDB Dataset Solubility Probability Density and Kernel Density Estimate Overlay')

# Show the plot
plt.show()





# Get 3 lowest and 3 highest solubilities
aq_sol_dataset_sorted: DataFrame = aq_sol_dataset.sort_values('Solubility')
three_highest_lowest_sol: DataFrame = pd.concat([aq_sol_dataset_sorted[:3], aq_sol_dataset_sorted[-3:]])

# Create list of strings for legend text
legend_text: List = [
    f"{compound.ID}: solubility = {compound.Solubility:.2f}" for compound in three_highest_lowest_sol.itertuples()
]

# Plot compounds on a grid
three_highest_lowest_sol_mols: List[Mol] = [
    rdkit.Chem.MolFromInchi(inchi) for inchi in three_highest_lowest_sol.InChI
]
rdkit.Chem.Draw.MolsToGridImage(
    three_highest_lowest_sol_mols,
    molsPerRow=3,
    subImgSize=(250, 250),
    legends=legend_text
)








aq_sol_dataset_features: int = list(aq_sol_dataset.columns).index("MolWt")
aq_sol_dataset_feature_names: List = aq_sol_dataset.columns[aq_sol_dataset_features:]

fig, axes = plt.subplots(nrows=5, ncols=4, sharey=True, figsize=(12, 8), dpi=300)

# Flatten 5x4 grid of subplots into a 1D array for easier iteration
axes: np.ndarray[Axes] = axes.flatten()

for key, feature_name in enumerate(aq_sol_dataset_feature_names):
    ax: Axes = axes[key]
    ax.scatter(
        aq_sol_dataset[feature_name],
        aq_sol_dataset.Solubility,
        s=6,
        alpha=0.4,
        color=f"C{key}"
    )
    if key % 4 == 0:
        ax.set_ylabel('Solubility')
    ax.set_xlabel(feature_name)

for i in range(len(aq_sol_dataset_feature_names), len(axes)):
    fig.delaxes(axes[i])

plt.tight_layout()
plt.show()








# Test linear regression utility function
linear_regression_utils.linear_regression(
    feature_vector=np.array([1, 0, 2.5]),
    weight_vector=np.array([0.2, -0.5, 0.4]),
    bias_term = 4.3
)





# Extract features and labels from dataset
aq_sol_feature_values: np.ndarray  = aq_sol_dataset.loc[:, aq_sol_dataset_feature_names].values
aq_sol_label_values: np.ndarray  = aq_sol_dataset.Solubility.values

# Calculate number of features (feature dimensionality)
aq_sol_feature_dim: int = aq_sol_feature_values.shape[1]

# Initialise the weight vector with random values from a normal distribution. 
# The size of the vector is equal to the number of features
weight_vector: Union[np.ndarray, ArrayImpl] = np.random.normal(size=aq_sol_feature_dim)

# Initialise the bias term as 0.0
bias_term: Union[float, ArrayImpl] = 0.0

linear_regression_utils.mean_squared_error_loss_wrapper(
    weight_vector,
    bias_term,
    aq_sol_feature_values,
    aq_sol_label_values
)








# Instantiate `loss_grad` which is an instance of `jax.grad()` that calculates 
# the analytical derivative of arguments 0 (weight vector) and 1 (bias term) of the 
# `mean_squared_error_loss_wrapper()` function
loss_grad: Callable = jax.grad(linear_regression_utils.mean_squared_error_loss_wrapper, (0, 1))

loss_grad(
    weight_vector, 
    bias_term, 
    aq_sol_feature_values, 
    aq_sol_label_values
)











loss_progress: List = []
learning_rate: float = 1e-6
for i in range(100):
    # Calculate gradients w.r.t weights and bias term
    grad: Tuple = loss_grad(
        weight_vector, 
        bias_term, 
        aq_sol_feature_values, 
        aq_sol_label_values
    )

    # Update weight vector by subtracting the product of the learning rate and the gradient
    # w.r.t the weights. This moves the weights in the direction of the negative gradient,
    # reducing the loss
    weight_vector -= learning_rate  * grad[0]

    # Similarly, update bias term by subtracting the product of the learning rate and the 
    # gradient w.r.t the bias
    bias_term -= learning_rate  * grad[1]

    # Calculate new MSE loss on each iteration to track loss progress
    loss_progress.append(
        linear_regression_utils.mean_squared_error_loss_wrapper(
            weight_vector, 
            bias_term,
            aq_sol_feature_values,
            aq_sol_label_values
        )
    )
    
plt.plot(loss_progress)
plt.xlabel("Step")
plt.yscale("log")
plt.ylabel("Loss")
plt.title("AqSolDB Training Curve")
plt.show()





# Initialise a the weight vector with random values from a normal distribution. 
# The size of the vector is equal to the number of features
weight_vector: Union[np.ndarray, ArrayImpl] = np.random.normal(size=aq_sol_feature_dim)

# Initialise the bias term as 0.0
bias_term: Union[float, ArrayImpl] = 0.0

loss_progress: List = []
learning_rate: float = 1e-6
batch_size: int = 32
num_data_points: int = len(aq_sol_label_values)

# Prepare batches
#
# Calculate largest multiple of batch size that fits into dataset, ensuring data is evenly
# distributed into batches, and dropping any left over data. This leads to more consistent
# and stable updates
num_batch_data: int = num_data_points // batch_size * batch_size

# Reshape `aq_sol_feature_values` and `aq_sol_label_values` matrices/DataFrames to have 
# dimensions of `(-1, batch_size, feature_dim)`. The `-1` allows Numpy to automatically
# calculate the appropriate size for this dimension
aq_sol_batched_features: np.array = aq_sol_feature_values[:num_batch_data].reshape(
    (-1, batch_size, aq_sol_feature_dim)
)
aq_sol_batched_labels: np.array = aq_sol_label_values[:num_batch_data].reshape(
    (-1, batch_size)
)

# Iterate over batches randomly for stochastic gradient descent
indices: np.array = np.arange(num_batch_data // batch_size)
np.random.shuffle(indices)

for i in indices:
    # Calculate gradients w.r.t weights and bias term
    grad: Tuple = loss_grad(
        weight_vector, 
        bias_term, 
        aq_sol_batched_features[i], 
        aq_sol_batched_labels[i]
    )

    # Update weight vector by subtracting the product of the learning rate and the gradient
    # w.r.t the weights. This moves the weights in the direction of the negative gradient,
    # reducing the loss
    weight_vector -= learning_rate  * grad[0]

    # Similarly, update bias term by subtracting the product of the learning rate and the 
    # gradient w.r.t the bias
    bias_term -= learning_rate  * grad[1]

    # Calculate new MSE loss on each iteration to track loss progress every 10 steps
    if i % 10 == 0:
        loss_progress.append(
            linear_regression_utils.mean_squared_error_loss_wrapper(
                weight_vector, 
                bias_term,
                aq_sol_feature_values,
                aq_sol_label_values
            )
        )

# Scale x-axis coordinates by 10 to include every step as we only record loss every 10 steps. 
# Keep y-axis coordinates the same
plt.plot(np.arange(len(loss_progress)) * 10, loss_progress)
plt.xlabel("Step")
plt.yscale("log")
plt.ylabel("Loss")
plt.title("Batched AqSolDB Training Curve")
plt.show()








# Calculate feature standard deviation & mean
aq_sol_feature_std: np.ndarray = np.std(aq_sol_feature_values, axis=0)
aq_sol_feature_mean: np.ndarray = np.mean(aq_sol_feature_values, axis=0)

# Standardise the features
aq_sol_std_features: np.ndarray = (aq_sol_feature_values - aq_sol_feature_mean) / aq_sol_feature_std

# Initialise a the weight vector with random values from a normal distribution. 
# The size of the vector is equal to the number of features
# We have to scale the initial weights by 0.1 so that the dot product of the weights and the new
# standardised features is not too small or too large
weight_vector: Union[np.ndarray, ArrayImpl] = np.random.normal(scale=0.1, size=aq_sol_feature_dim)

# Initialise the bias term as 0.0
bias_term: float = 0.0

# Initialise number of epochs (how many times to iterate over all dataset batches
num_epochs: Union[float, ArrayImpl] = 3

loss_progress: List = []
learning_rate: float = 1e-2
batch_size: int = 32
num_data_points: int = len(aq_sol_label_values)

# Reshape `aq_sol_std_features` and `aq_sol_label_values` matrices/DataFrames to have 
# dimensions of `(-1, batch_size, feature_dim)`. The `-1` allows Numpy to automatically
# calculate the appropriate size for this dimension
aq_sol_batched_features: np.ndarray = aq_sol_std_features[:num_batch_data].reshape(
    (-1, batch_size, aq_sol_feature_dim)
)
aq_sol_batched_labels: np.ndarray = aq_sol_label_values[:num_batch_data].reshape(
    (-1, batch_size)
)

indices: np.ndarray = np.arange(num_batch_data // batch_size)

# Iterate over all dataset batches 3 times
for epoch in range(num_epochs):
    
    # Iterate over batches randomly for stochastic gradient descent
    np.random.shuffle(indices)
    for i in indices:
        # Calculate gradients w.r.t weights and bias term
        grad: Tuple = loss_grad(
            weight_vector, 
            bias_term, 
            aq_sol_batched_features[i], 
            aq_sol_batched_labels[i]
        )
    
        # Update weight vector by subtracting the product of the learning rate and the gradient
        # w.r.t the weights. This moves the weights in the direction of the negative gradient,
        # reducing the loss
        weight_vector -= learning_rate  * grad[0]
    
        # Similarly, update bias term by subtracting the product of the learning rate and the 
        # gradient w.r.t the bias
        bias_term -= learning_rate  * grad[1]
    
        # Calculate new MSE loss on each iteration to track loss progress every 50 steps
        if i % 50 == 0:
            loss_progress.append(
                linear_regression_utils.mean_squared_error_loss_wrapper(
                    weight_vector, 
                    bias_term,
                    aq_sol_std_features,
                    aq_sol_label_values
                )
            )
            
# Scale x-axis coordinates by 50 to include every step as we only record loss every 50 steps. 
# Keep y-axis coordinates the same
plt.plot(np.arange(len(loss_progress)) * 50, loss_progress)
plt.xlabel("Step")
plt.yscale("log")
plt.ylabel("Loss")
plt.title("Standardised Batched AqSolDB Training Curve")
plt.show()








linear_regression_predicted_labels: ArrayImpl = linear_regression_utils.linear_regression(
    aq_sol_std_features,
    weight_vector,
    bias_term
)

# Plot 45-deg reference line from (-100, -100) to (100, 100)
plt.plot([-100, 100], [-100, 100])

plt.scatter(aq_sol_label_values, linear_regression_predicted_labels, s=4, alpha=0.7)
plt.xlabel(r"Measured Solubility $y$")
plt.ylabel(r"Predicted Solubility $\hat{y}$")
plt.xlim(-13.5, 2)
plt.ylim(-13.5, 2)
plt.show()





# Slice correlation between predicted labels and actual labels from correlation matrix
np.corrcoef(aq_sol_label_values, linear_regression_predicted_labels)[0, 1]



